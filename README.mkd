# Pachama
- Have you ever wanted to return arrays full of pixel values from a raster file? Ever thought with so many bands where do I even start? Well... Look no further, because that is actually the only thing this repo does! 

- One note about the main function `extract_chips_multi_point` in the python module `extract_chips` in relation to the instructions. The instructions were very specific about what the function should expect and return. I tweaked this definition slightly where the function returns a dictionary with each key representing a specific coordinate. The value of that key is a list of chips, one per year. Each chip is a numpy array of shape [6, 2*chip_size + 1, 2*chip_size + 1]. 

# To Run

- To run the function, please ensure that the paths exist for the `extract_chips_multi_point` in terms of tif files and coordinates. 

# Questions 
1. Informally describe the runtime and memory complexities of your solution as a function of: 

- The number of raster files 
- The number of coordinates 

The runtime of my solution increases linearly as a function of the number of raster files, but with the addition of the coordinates since it is a for loop in a for loop, this makes the algorithm O(n^2). At a quick glance running over one tif file takes 4.5 seconds while running over 2 takes 7.5. The number of points is also a linear in terms of runtime, however it takes much less memory per unit as it efficiently creates a window in the raster file and only loads a small amount of data. This process could be made more efficient if the code was structured to run concurrently and process multiple coordinates at a time.This could be done with the python's multiprocessing module and stripping the function of its for-loop.

In terms of memory complexity - the big deal is the raster files, since as stated above as the number of coordinates increases the number only small portions of that file will be read. However it is my understanding that the raster file needs to be held in memory, unsure how the performance would be effected if you tried to hold more than one raster file in memory.

2. What are the performance and scalability concerns with you solution? 

The largest concern is that the method cannot gain efficiency by larger computational power or RAM. Its a simple for-loop that iterates over the coordinates and tif files, and so adding more horse power wouldn't gain very much in terms of efficiency. The process needs to be changes to be scalable so that multiple jobs can run concurrently so adding that additional RAM or compute will make a difference.  



3. How would an ideal implementation change if you had to process 100x the number of rasters and/or coordinates? 
 - Ideally a GPU instead of a CPU should be used. There is a blatent lack of concurreny in my method and using as mentioned in the bonus questions some process like dask would be beneficial. In addition this system should be set up in the cloud so that scalability is less of a concern in terms of ability and minimizing cost becomes the focus. Databricks, Sagemaker, GCP's Vertex or Azure ML Studio allow individuals to spin up GPU enabled clusters to run such large scale image and matrix operations. 



## Run and test with pip

- run `pip install raster-analysis==0.1.0`
- run the following in a python module and ensure data directories exist.
```
from raster_analysis.extract_chips import extract_chips_multi_point

geo_json = "./pachama-interview-data/coordinates.geojson"
raster_dir = "./pachama-interview-data/rasters/"
chip_size = 4
test_run = extract_chips_multi_point(geo_json, raster_dir, chip_size)
test_coordinate = (-74.79528902505427, 40.946807786177864)
print(f"Type of object returned: {type(test_run)}")
print(f"Type of object in dictionary: {type(test_run[test_coordinate])}")
print(f"Length of list: {len(test_run[test_coordinate])}")
print(f"Type item in list: {type(test_run[test_coordinate][0])}")
print(f"Shape array in list: {test_run[test_coordinate][0].shape}")

```

## First Time Setup to Build locally

0. __Get data from Pachama.__

- `mkdir pachama_interview_data/`

- `gsutil -m cp -R gs://pachama-interview-data/* pachama_interview_data/`


1. __Create a virtual environment.__ The following command will create a vanilla python environment in a 'venv' directory. This directory will not be synced to github. Any required packages will be installed into this virtual environment.

`pachama>python -m venv venv`

2. __Install required packages into the virtual environment.__ Packages that have been installed during development are captured in the 'pachama.requirements.txt' file. The following command will read this file and install all the required packages in your environment:

`pachama>pip install -r requirements.txt`

## Every Time Setup
Before doing any development work, activate your virtual environment:

`pachama> source < location-of-project >/venv/bin/activate`

## Running PyTest
The non-obvious inspector functions have companion tests in the __tests__ directory.

You can run the tests with the following command:

`~\pachama>python -m pytest`

Youd can also build and see a test coverage report with the following commands:

`~\pachama>coverage run -m pytest `
## Adding Packages
If you have to add a new package to support development, do the following.
1. Ensure that existing required packages have been installed:

`pachama>pip install -r requirements.txt`

2. Update the requirements.txt file from your virtual environment:

`pachama>pip freeze > requirements.txt`

`pachama> coverage report`

## Current Coverage
- raster_analysis/extract_chips.py: 76%
- tests/test_extract_chips.py:      100%
- tests/test_validate_path.py:      93%
- utilities/validate_path.py:       94%
## Sphinx Documentation (COMING SOON!)
1. Navigate to docs folder:

`~\docs>cd ~\pachama\docs`

2. Generate api documentation:

`~\docs>sphinx-apidoc -o source ..\pachama`

3. Rebuild documentation

`~\docs>make html`



