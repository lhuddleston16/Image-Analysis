# Pachama
- Have you ever wanted to return arrays full of pixel values from a raster file? Ever thought with so many bands where do I even start? Well... Look no further, because that is actually the only thing this repo does! 

- One note about the main function `extract_chips_multi_point` in the python module `extract_chips` in relation to the instructions. The instructions were very specific about what the function should expect and return. I tweaked this definition slightly where the function returns a dictionary with each key representing a specific coordinate. The value of that key is a list of chips, one per year. Each chip is a numpy array of shape [6, 2*chip_size + 1, 2*chip_size + 1]. 

# To Run

- To run the function, please ensure that the paths exist for the `extract_chips_multi_point` in terms of tif files and coordinates. 

# Questions 
1. Informally describe the runtime and memory complexities of your solution as a function of: 

- The number of raster files 
- The number of coordinates 

The runtime of my solution increases linearly as a function of the number of raster files. At a quick glance running over one tif file takes 4.5 seconds while running over 2 takes 7.5. Conceptually linear time makes makes sense as well since this function iterates over both tif images and coordinates. Unsure at this time if there are gains in efficiency as more rasters are added, but I doubt it based on the code. The number of points also appears to be a linear in terms of runtime, however it takes much less memory per unit as it efficiently creates a window in the raster file and only loads a small amount of data. This process could be made sub-linear if the code was structured to run concurrently and process multiple coordinates at a time.

In terms of memory complexity - the big deal is the raster files, since as stated above as the number of coordinates increases the number only small portions of that file will be read. However it is my understanding that the raster file needs to be held in memory, unsure how the performance would be effected if you tried to hold more than one raster file in memory.

2. What are the performance and scalability concerns with you solution? 

The largest concern is that the method cannot gain efficiency by larger computational power or RAM. Its a simple for-loop that iterates over the coordinates and tif files, and so adding more horse power wouldn't gain very much in terms of efficiency. The process needs to be changes to be scalable so that multiple jobs can run concurrently so adding that additional RAM or compute will make a difference.  



3. How would an ideal implementation change if you had to process 100x the number of rasters and/or coordinates? 
 - Ideally a gpu instead of a CPU should be used. There is a blatent lack of concurreny in my method and using as mentioned in the bonus questions some process like dask would be beneficial. In addition this system should be set up in the cloud so that scalability is less of a concern in terms of ability and minimizing cost becomes the focus. Databricks, Sagemaker, GCP's Vertex or Azure ML Studio allow individuals to spin up GPU enabled clusters to run such large scale image and matrix operations. The way this could be done is to deploy a docker image unto those clusers.



## First Time Setup

0. __Get data from Pachama.__

- `mkdir pachama_interview_data/`

- `gsutil -m cp -R gs://pachama-interview-data/* pachama_interview_data/`


1. __Create a virtual environment.__ The following command will create a vanilla python environment in a 'venv' directory. This directory will not be synced to github. Any required packages will be installed into this virtual environment.

`pachama>python -m venv venv`

2. __Install required packages into the virtual environment.__ Packages that have been installed during development are captured in the 'pachama.requirements.txt' file. The following command will read this file and install all the required packages in your environment:

`pachama>pip install -r requirements.txt`

## Every Time Setup
Before doing any development work, activate your virtual environment:

`pachama> source < location-of-project >/venv/bin/activate`

## Running PyTest
The non-obvious inspector functions have companion tests in the __tests__ directory.

You can run the tests with the following command:

`~\pachama>python -m pytest tests\test.py`

Youd can also build and see a test coverage report with the following commands:

`~\pachama>coverage run -m pytest `
## Adding Packages
If you have to add a new package to support development, do the following.
1. Ensure that existing required packages have been installed:

`pachama>pip install -r requirements.txt`

2. Update the requirements.txt file from your virtual environment:

`pachama>pip freeze > requirements.txt`

`pachama> coverage report`

## Current Coverage
raster_analysis/__init__.py            0      0   100%
raster_analysis/extract_chips.py      41     10    76%
tests/test_extract_chips.py           31      0   100%
tests/test_validate_path.py           28      2    93%
utilities/__init__.py                  0      0   100%
utilities/validate_path.py            18      1    94%
## Sphinx Documentation (COMING SOON!)
1. Navigate to docs folder:

`~\docs>cd ~\pachama\docs`

2. Generate api documentation:

`~\docs>sphinx-apidoc -o source ..\pachama`

3. Rebuild documentation

`~\docs>make html`



